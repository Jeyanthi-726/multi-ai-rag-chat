# -*- coding: utf-8 -*-
"""Multi AI Chat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_mDUkqjP7STxoDNhCivNSkpX9pyR-wWm
"""



!pip install -q cassio datasets langchain openai tiktoken

!pip install -U langchain-community

# LangChain components to use
from langchain.vectorstores.cassandra import Cassandra
from langchain.indexes.vectorstore import VectorStoreIndexWrapper
from langchain.llms import OpenAI
from langchain.embeddings import OpenAIEmbeddings

# Support for dataset retrieval with Hugging Face
from datasets import load_dataset

# With CassIO, the engine powering the Astra DB integration in LangChain,
# you will also initialize the DB connection:
import cassio

!pip install PyPDF2

from PyPDF2 import PdfReader

import cassio
## connection of the ASTRA DB
ASTRA_DB_APPLICATION_TOKEN="Astra Token" # enter the "AstraCS:..." string found in in your Token JSON file"
ASTRA_DB_ID=" data base id"
cassio.init(token=ASTRA_DB_APPLICATION_TOKEN,database_id=ASTRA_DB_ID)

!pip install langchain_community

!pip install -U langchain_community tiktoken langchain-groq langchainhub chromadb langchain langgraph langchain_huggingface

### Build Index

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma


### from langchain_cohere import CohereEmbeddings



# Docs to index
urls = [
    "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/",
    "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/",
]

# Load
docs = [WebBaseLoader(url).load() for url in urls]
docs_list = [item for sublist in docs for item in sublist]

# Split
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=500, chunk_overlap=0
)
doc_splits = text_splitter.split_documents(docs_list)

!pip install langchain_huggingface

from langchain_huggingface import HuggingFaceEmbeddings
embeddings=HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

from langchain.vectorstores.cassandra import Cassandra
astra_vector_store=Cassandra(
    embedding=embeddings,
    table_name="rag_chat",
    session=None,
    keyspace=None

)

from langchain.indexes.vectorstore import VectorStoreIndexWrapper
astra_vector_store.add_documents(doc_splits)
print("Inserted %i headlines." % len(doc_splits))

astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)

retriever=astra_vector_store.as_retriever()

# Number of results to retrieve
top_k = 5
query = "What is agent memory?"

# Perform similarity search with scores
results_with_scores = astra_vector_store.similarity_search_with_score(query, k=top_k)

# Display results
for i, (doc, score) in enumerate(results_with_scores):
    print(f"\nüîπ Result #{i+1}")
    print(f"Similarity Score: {score:.4f}")
    print("Document Content:\n", doc.page_content)

retriever.invoke("What is agent",ConsistencyLevel="LOCAL_ONE")

!pip install langchain_groq

### Router

from typing import Literal

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field

### 3. Router LLM
from google.colab import userdata
import os
from langchain_groq import ChatGroq
groq_api_key = userdata.get("groq_api_key")
os.environ["GROQ_API_KEY"] = groq_api_key
llm = ChatGroq(groq_api_key=groq_api_key, model_name="Gemma2-9b-It")

# """
# You are a router that decides where to send a user question. You have access to:
# - VECTORSTORE (for agent-based content, prompt engineering, LLM attacks, agent memory, etc.)
# - WIKIPEDIA (general world info like people, places, history, companies)
# - ARXIV (technical questions, new research topics, or papers)

class RouteQuery(BaseModel):
    datasource: Literal["vectorstore", "wiki_search", "arxiv_search"] = Field(...)

router_prompt = ChatPromptTemplate.from_messages([
    ("system","""
You are an expert router deciding the best knowledge source for answering questions.
You can route to one of the following:
- VECTORSTORE: Questions about agents, prompt engineering, agent memory, adversarial attacks (from Lilian Weng's blog).
- WIKIPEDIA: General knowledge, people, events, companies, movies, etc.
- ARXIV: Research-oriented queries, latest papers, surveys, model architecture , benchmarks.

Respond only with one of: 'vectorstore', 'wiki_search', or 'arxiv_search'. """),
    ("human", "{question}")
])
structured_llm_router = llm.with_structured_output(RouteQuery)
question_router = router_prompt | structured_llm_router

!pip install langchain_community langgraph arxiv wikipedia

### 4. Tools
from langchain_community.utilities import ArxivAPIWrapper, WikipediaAPIWrapper
from langchain_community.tools import ArxivQueryRun, WikipediaQueryRun
wiki = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=200))
arxiv = ArxivQueryRun(api_wrapper=ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200))

### 5. Define Graph State with Memory
class GraphState(TypedDict):
    question: str
    generation: str
    documents: List[Document]
    chat_history: List[str]  # ‚úÖ MEMORY

from langchain_core.pydantic_v1 import BaseModel, Field
from typing import Literal

class RouteQuery(BaseModel):
    datasource: Literal["vectorstore", "wiki_search", "arxiv_search"] = Field(...)

structured_llm_router = llm.with_structured_output(RouteQuery)
question_router = router_prompt | structured_llm_router

### 6. Nodes

# def route_question(state):
#     print("--- ROUTE ---")
#     question = state["question"]
#     chat_history = state.get("chat_history", [])
#     full_context = "\n".join(chat_history + [question])
#     source = question_router.invoke({"question": full_context})
#     print(f"Routing to: {source.datasource}")
#     return source.datasource


# def route_question(state):
#     question = state["question"]
#     chat_history = state.get("chat_history", [])
#     full_context = "\n().join(chat_history + [question])

#     # Hard rule override (production fallback)
#     keywords_vector = ["agent", "prompt", "adversarial", "memory"]
#     keywords_arxiv = ["research", "paper", "transformer", "architecture", "LLM survey"]

#     q_lower = question.lower()

#     if any(k in q_lower for k in keywords_vector):
#         return "vectorstore"
#     elif any(k in q_lower for k in keywords_arxiv):
#         return "arxiv_search"

#     # LLM fallback (when no keywords hit)
#     source = question_router.invoke({"question": full_context})
#     return source.datasource

# def route_question(state):
#     """
#     Route question to wiki search or RAG.

#     Args:
#         state (dict): The current graph state

#     Returns:
#         str: Next node to call
#     """

#     print("---ROUTE QUESTION---")
#     question = state["question"]
#     source = question_router.invoke({"question": question})
#     if source.datasource == "wiki_search":
#         print("---ROUTE QUESTION TO Wiki SEARCH---")
#         return "wiki_search"
#     elif source.datasource == "arxiv_search":
#         print("---ROUTE QUESTION TO arxiv_search---")
#         return "arxiv_search"
#     elif source.datasource == "vectorstore":
#         print("---ROUTE QUESTION TO RAG---")
#         return "vectorstore"

def route_question(state):
    question = state["question"]
    chat_history = state.get("chat_history", [])
    full_context = "\n".join(chat_history + [question])

    print("--- ROUTE ---")
    source = question_router.invoke({"question": full_context})
    print(f"Routing to: {source.datasource}")
     # ‚úÖ Logging
    print(f"[ROUTED] Question: '{question}'")
    print(f"[ROUTED] Full Context: {full_context}")
    print(f"[ROUTED] Chosen Source: {source.datasource}")
    return source.datasource



def wiki_search(state):
    print("--- WIKIPEDIA ---")
    question = state["question"]
    result = wiki.invoke({"query": question})
    return {"documents": [Document(page_content=result)], "question": question, "chat_history": state["chat_history"]}

def arxiv_search(state):
    print("--- ARXIV ---")
    question = state["question"]
    result = arxiv.invoke({"query": question})
    return {"documents": [Document(page_content=result)], "question": question, "chat_history": state["chat_history"]}

def retrieve(state):
    print("--- RAG RETRIEVAL ---")
    question = state["question"]
    docs = retriever.invoke(question)
    return {"documents": docs, "question": question, "chat_history": state["chat_history"]}

def generate_answer(state):
    print("--- GENERATE FINAL ANSWER ---")
    question = state["question"]
    chat_history = state.get("chat_history", [])
    docs = state["documents"]
    history_str = "\n".join(chat_history)
    context = "\n\n".join([doc.page_content for doc in docs])

    prompt = f"""You are a helpful assistant.

Conversation so far:
{history_str}

Now answer this:
Question: {question}

Context to help you:
{context}

Answer:"""

    response = llm.invoke(prompt)
    updated_history = chat_history + [question]

    return {
        "question": question,
        "documents": docs,
        "generation": response.content,
        "chat_history": updated_history
    }

from langchain.schema import Document
from langgraph.graph import END, StateGraph, START
from typing_extensions import TypedDict
from typing import List, Literal
import os
from google.colab import userdata
from pprint import pprint

### 7. Build Graph
workflow = StateGraph(GraphState)
workflow.add_node("wiki_search", wiki_search)
workflow.add_node("arxiv_search", arxiv_search)
workflow.add_node("retrieve", retrieve)
workflow.add_node("generate_answer", generate_answer)

workflow.add_conditional_edges(START, route_question, {
    "wiki_search": "wiki_search",
    "arxiv_search": "arxiv_search",
    "vectorstore": "retrieve"
})
workflow.add_edge("wiki_search", "generate_answer")
workflow.add_edge("arxiv_search", "generate_answer")
workflow.add_edge("retrieve", "generate_answer")
workflow.add_edge("generate_answer", END)

app = workflow.compile()

from IPython.display import Image, display

try:
    display(Image(app.get_graph().draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass

conversation_history = []

# üîπ Q1: General knowledge (Wikipedia)
inputs = {"question": "Who is Elon Musk?", "chat_history": conversation_history}
for output in app.stream(inputs):
    for key, value in output.items(): pprint(f"Node '{key}':")
conversation_history = value["chat_history"]
print("‚úÖ Answer:")
pprint(value["generation"])

# üîπ Q2: Follow-up (still Wikipedia)
inputs = {"question": "What companies did he start?", "chat_history": conversation_history}
for output in app.stream(inputs):
    for key, value in output.items(): pprint(f"Node '{key}':")
conversation_history = value["chat_history"]
print("‚úÖ Answer:")
pprint(value["generation"])

# Deeper follow-up
inputs = {"question": "What is his goal with Mars?", "chat_history": conversation_history}
for output in app.stream(inputs):
    for key, value in output.items(): pprint(f"Node '{key}':")
conversation_history = value["chat_history"]
print("‚úÖ Q3 Answer:")
pprint(value["generation"])

print("Chat History:")
for turn in conversation_history:
    print("-", turn)

# üîπ Q3: RAG-based from blog
inputs = {"question": "What is agent memory?", "chat_history": conversation_history}
for output in app.stream(inputs):
    for key, value in output.items(): pprint(f"Node '{key}':")
conversation_history = value["chat_history"]
print("‚úÖ Answer:")
pprint(value["generation"])

def test_routing(question: str, expected: str):
    result = question_router.invoke({"question": question})
    actual = result.datasource
    status = "‚úÖ PASS" if actual == expected else "‚ùå FAIL"
    print(f"{status} | Q: {question} | Expected: {expected} | Got: {actual}")

# VECTORSTORE tests (from Lilian Weng blog topics)
test_routing("What is an agent in AI?", "vectorstore")
test_routing("Explain agent memory types", "vectorstore")
test_routing("How do adversarial attacks affect LLMs?", "vectorstore")
test_routing("What is prompt engineering?", "vectorstore")

# WIKIPEDIA tests
test_routing("Who is Virat Kohli?", "wiki_search")
test_routing("What is India?", "wiki_search")
test_routing("When was Google founded?", "wiki_search")
test_routing("Where is Eiffel Tower?", "wiki_search")

# ARXIV tests
test_routing("Recent papers on retrieval-augmented generation", "arxiv_search")
test_routing("What are the latest papers on transformers?", "arxiv_search")
test_routing("Benchmark results for LLaMA 3", "arxiv_search")
test_routing("Survey on multimodal LLMs", "arxiv_search")

test_routing("Types of agents used in LLM systems", "vectorstore")
test_routing("How does reflection work in AI agents?", "vectorstore")
test_routing("Describe LLM agent planning with ReAct", "vectorstore")
test_routing("Explain recursive summarization in prompt engineering", "vectorstore")

test_routing("What is the Taj Mahal?", "wiki_search")
test_routing("When was Apple Inc. founded?", "wiki_search")
test_routing("Who is the Prime Minister of the UK?", "wiki_search")
test_routing("What is the capital of Japan?", "wiki_search")

test_routing("Latest research on LLM memory mechanisms", "arxiv_search")
test_routing("Papers about diffusion models in NLP", "arxiv_search")
test_routing("Compare LLaMA-2 and GPT-4 architecture", "arxiv_search")
test_routing("Transformer-based summarization benchmarks 2024", "arxiv_search")

test_routing("What is memory?", "wiki_search")  # Could go to wiki or vector
test_routing("What is a prompt?", "wiki_search")  # Too generic ‚Äî test behavior
test_routing("Explain memory", "wiki_search")     # Missing context

# Change expectations to reflect how router treats domain-sensitive terms
test_routing("What is memory?", "vectorstore")
test_routing("What is a prompt?", "vectorstore")
test_routing("Explain memory", "vectorstore")